<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

Timeline:

<div class="column-1 flex-col">
    <div class="figure" style="grid-template-columns: 33% 33% 33%; width: 90%;">
        <img src="../../assets/2018_gan_stability/history/gan_t0.png" width=80%></img>
        <p>[Goodfellow et al., 2014]</p>
        <img src="../../assets/2018_gan_stability/history/gan_t1.png" width=80%></img>
        <p>[Radford & Metz, 2016]</p>
        <img src="../../assets/2018_gan_stability/history/gan_t2.png" width=80%></img>
        <p>[Karras et al., 2018]</p>
    </div>
    <img src="../../assets/2018_gan_stability/history/arrow.svg" width=90%></div>
</div>

---

<!-- .slide: class="layout-2x1" -->

## Generative Adversarial Networks

<div class="column-1">
    <h3>Generative Adversarial Networks:</h3>
    <ul>
        <li class="fragment">We can now achieve impressive results</li>
        <li class="fragment">However, training is still difficult and requires many heuristics</li>
    </ul>
</div>

<div class="column-2 fragment">
    <h3>Questions:</h3>
    <ul>
        <li class="fragment">What are the underlying principles?</li>
        <li class="fragment">Can we give theoretical guarantees for convergence?</li>
        <li class="fragment">Can we derive new families of algorithms?</li>
    </ul>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

<h3>Alternating Gradient Descent:</h3>

<div class="column-1 flex-col">
    <img src="../../assets/2018_gan_stability/algorithm/altgd.svg" width="40%"></img>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

<h3>Simultaneous Gradient Descent:</h3>

<div class="column-1 flex-col">
    <img src="../../assets/2018_gan_stability/algorithm/simgd.svg" width="40%"></img>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

<h3>What are we doing here?</h3>

<div class="column-1 flex-col" style="justify-content: space-evenly;">
    <ul>
        <li class="fragment">We no longer solve an optimization problem</li>
        <li class="fragment">Instead, we solve a smooth two player</li>
    </ul>
    <div class="flex-row box-highlighted fragment" 
        style="width: 70%; align-self: center; text-align: left;">
        <img src="../../assets/misc/symbols/arrow.svg" width="20%">
        <div style="padding: 0px 50px;">
            Standard convergence results for stochastic gradient descent hence no longer apply
        </div>
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

<h3>Questions:</h3>

<div class="column-1">
    <ol style="width: 80%">
        <li class="fragment">
            Does a (pure) Nash-equilibrium exist?
            <ul class="fragment"><li>
                Yes, if there is $\theta$ with $p_\theta = p_{\mathcal D}$
            </li></ul>
        </li>
        <li class="fragment">
            Does it solve the min-max problem?
            <ul class="fragment"><li>
                Yes, if $p_\theta = p_{\mathcal D}$
            </li></ul>
        </li>
        <li class="box-highlighted fragment">
            <div>
                Do simultaneous and / or alternating gradient descent converge to the Nash-equilibrium?
            </div>
        </li>
    </ol>
</div>
