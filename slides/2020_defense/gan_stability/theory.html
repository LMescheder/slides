<!-- .slide: class="layout-1x1" -->

## Convergence Theory

<div class="column-1 flex-col">
    <div style="width: 80%; padding: 30px;">
        <q>
            Simple experiments, simple theorems are the building blocks that help us understand more complicated systems.   
        </q>
    </div>
    <p style="font-size: 80%">Ali Rahimi â€“ Test of Time Award speech, NeurIPS 2017</p>
</div>

---

<!-- .slide: class="layout-2x1" -->

## Convergence Theory

The Dirac-GAN

<div class="column-1 flex-col">
    <img src="assets/2018_gan_stability/dirac_gan/dirac.png" width="1000px">
</div>

<div class="column-2 flex-col fragment">
    <div class="flex-row" style="width: 60%; align-self: center;">
        <div>$p_{\mathcal D} = \delta_0$</div>
        <div>$p_{\theta} = \delta_\theta$</div>
        <div>$D_\psi(x) = \psi \cdot x$</div>
    </div>
    <div class="flex-row" style="padding: 50px;">
        $L(\theta, \psi) = f(\theta\cdot \psi) + f(0)$
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

Unregularized GAN-Training:

<div class="column-1 flex-col">
    <video data-autoplay loop src="assets/2018_gan_stability/dirac_gan/gan.webm" width="90%">
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

<div class="column-1 flex-col">
    <div>
        Understanding the <strong>gradient vector field</strong>:
        $$
            v(\theta, \psi)
            =
            \begin{pmatrix}
            - \nabla_\theta L(\theta, \psi) \\
            \nabla_\psi L(\theta, \psi)
            \end{pmatrix}
        $$
    </div>
    <div class="fragment">
        <strong>Local convergence</strong> of simultaneous and
        alternating gradient descent determined by <strong>eigenvalues</strong>
        of the Jacobian
        $$
            v^\prime(\theta^*, \psi^*)
            =
            \begin{pmatrix}
            - \nabla^2_\theta L(\theta, \psi)
            & -\nabla_{\theta,\psi} L(\theta, \psi) \\
            \nabla_{\theta,\psi} L(\theta, \psi)
            & \nabla^2_\psi L(\theta, \psi)
            \end{pmatrix}
        $$
    </div>
</div>


---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

Continuous system:

<div class="column-1 flex-col">
    <div class="figure" style="grid-column-template: 50% 50%;">
        <img src="assets/2018_gan_stability/eigenvalues/eigval0.svg"
            style="padding: 0 0 20px 20px"
            width=750px>
        <img src="assets/2018_gan_stability/eigenvalues/eigval0_vis.png"
            style="padding: 50px 50px 0 0;"
            width=840px>
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

Continuous system:

<div class="column-1 flex-col">
    <div class="figure" style="grid-column-template: 50% 50%;">
        <img src="assets/2018_gan_stability/eigenvalues/eigval1.svg" 
            style="padding: 0 0 20px 20px"
            width=750px
        >
        <img src="assets/2018_gan_stability/eigenvalues/eigval1_vis.png" 
            style="padding: 50px 50px 0 0;"
            width=840px>
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

Discretized system:

<div class="column-1 flex-col">
    <div class="figure" style="grid-column-template: 50% 50%;">
        <img src="assets/2018_gan_stability/eigenvalues/eigval2.svg" 
            style="padding: 0 0 20px 20px"
            width=750px>
        <img src="assets/2018_gan_stability/eigenvalues/eigval2_vis.png" 
            style="padding: 50px 50px 0 0;"
            width=840>
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

Discretized system:

<div class="column-1 flex-col">
    <div class="figure" style="grid-column-template: 50% 50%;">
        <img src="assets/2018_gan_stability/eigenvalues/eigval3.svg" 
            style="padding: 0 0 20px 20px"
            width=750px >
        <img src="assets/2018_gan_stability/eigenvalues/eigval3_vis.png" 
            style="padding: 50px 50px 0 0;"
            width=840px>
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

<div class="subtitle" style="height: 150px">
    Unregularized GAN-Training:
</div>

<div class="column-1 flex-col">
    <div style="width: 85%">
        <video data-autoplay loop
            src="assets/2018_gan_stability/dirac_gan/gan.webm"
            width="100%">
    </div>
    <div>
        Eigenvalues: $\{-f^\prime(0)i, +f^\prime(0)i \}$
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

<div class="subtitle" style="height: 150px">
    Wasserstein-GAN training:
</div>

<div class="column-1 flex-col">
    <div style="width: 85%">
        <video data-autoplay loop
            src="assets/2018_gan_stability/dirac_gan/wgan1.webm"
            width="100%">
    </div>
    <div>
        Eigenvalues: $\{-i, +i \}$
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

<div class="subtitle" style="height: 150px">
    Wasserstein-GAN training:
    <p>(5 discriminator updates / generator update)</p>
</div>

<div class="column-1 flex-col">
    <div style="width: 85%">
        <video data-autoplay loop
            src="assets/2018_gan_stability/dirac_gan/wgan5.webm"
            width="100%">
    </div>
    <div>
        Eigenvalues: $\{-i, +i \}$
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

<div class="subtitle" style="height: 150px">
    Zero-centered gradient penalties
    <p><span style="font-size: 80%;">
        $
        R(\psi)
        :=
        \frac{\gamma}{2}
        \mathrm E_{p_{\mathcal D}(x)}
        \left[
        \|\nabla_x D_\psi(x) \|^2
        \right]
       $
    </span></p>
</div>

<div class="column-1 flex-col">
    <div style="width: 85%">
        <video data-autoplay loop
            src="assets/2018_gan_stability/dirac_gan/gan_gradpen.webm"
            width="100%">
    </div>
    <div>
        Eigenvalues: 
        <span style="font-size: 70%;">
            $
            \left\{
                -\frac{\gamma}{2} - \sqrt{\frac{\gamma^2}{4} - f^\prime(0)^2},
                -\frac{\gamma}{2} + \sqrt{\frac{\gamma^2}{4} - f^\prime(0)^2}
            \right\}
            $
        </span>
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Convergence Theory

<div class="subtitle" style="height: 150px">
    Zero-centered gradient penalties (critical)
    <p><span style="font-size: 80%;">
            $
            R(\psi)
            :=
            \frac{\gamma_{\text{critical}}}{2}
            \mathrm E_{p_{\mathcal D}(x)}
            \left[
            \|\nabla_x D_\psi(x) \|^2
            \right]
            $
        </span></p>
</div>

<div class="column-1 flex-col">
    <div style="width: 85%">
        <video data-autoplay loop
            src="assets/2018_gan_stability/dirac_gan/gan_gradpen.webm"
            width="100%">
    </div>
    <div>
        Eigenvalues:
        $
        \left\{
        -|f^\prime(0)|, -|f^\prime(0)|
        \right\}
        $
    </div>
</div>

---

<!-- .slide: class="layout-2x1" -->

## General Convergence Results


<div class="column-1">
    Regularizers for discriminator:
    $$\begin{split}
        R_1(\psi) & :=
            \frac{\gamma}{2}
            \mathrm E_{p_{\mathcal D}(x)}
            \left[
            \|\nabla_x D_\psi(x) \|^2
            \right]\\
        R_2(\theta, \psi) & :=
            \frac{\gamma}{2}
            \mathrm E_{p_\theta(x)}
            \left[
            \|\nabla_x D_\psi(x) \|^2
            \right]
    \end{split}$$
</div>

<div class="column-2 fragment">
    Regularized gradient vector field
    $$
        \tilde v_i(\theta, \psi)
        :=
        \begin{pmatrix}
        - \nabla_\theta L(\theta, \psi) \\
        \nabla_\psi L(\theta, \psi) - \nabla_\psi R_i(\theta, \psi)
        \end{pmatrix}
    $$
</div>

---

<!-- .slide: class="layout-1x1"" -->

## General Convergence Results


<div class="column-1 flex-col" style="width: 100%;">
    <div class="flex-col"
        style="align-self: center; flex: 1; width: 85%; justify-content: space-evenly; align-items: flex-start; text-align: left;">
        <div class="fragment">
            <strong>Assumption I</strong>:
            the generator can represent the true data distribution
        </div>
        <div class="fragment">
            <strong>Assumption II</strong>:
            $f^\prime(0) \neq 0$ and $f^{\prime\prime}(0) \leq 0$
        </div>
        <div class="fragment">
            <strong>Assumption III</strong>:
            the discriminator can detect when the generator deviates from the equilibrium
        </div>
        <div class="fragment">
            <span style="font-weight:bolder; text-decoration: line-through red;">
                <span style="font-weight:normal">
                    <strong>Assumption IV</strong>:
                    the generator and data distribution
                    have locally the same support (Nagarajan & Kolter)
                </span>
            </span>
        </div>
    </div>
</div>

---

<!-- .slide: class="layout-1x1"" -->

## General Convergence Results

<div class="column-1 flex-col">
    <div class="box-highlighted-strong" 
        style="width: 90%; text-align: left; padding: 50px; ">
        <strong>Theorem</strong>:
        under Assumption I, II, III and some mild technical assumptions the GAN training dynamics for the regularized training
        objective are locally asymptotically stable near the equilibrium point
    </div>
</div>


---

<!-- .slide: class="layout-1x1"" -->

## General Convergence Results

<div class="subtitle flex-row" style="padding: 0 50px;">
    <div style="text-align:left;">
        Proof (idea):
    </div>
    <div style="flex: 1; text-align:right;">
        (Extends prior work by Nagarajan & Kolter)
    </div>
</div>

<div class="column-1 flex-col">
    <div class="fragment">
        $$
        \tilde v_i(\theta, \psi)
        :=
        \begin{pmatrix}
        - \nabla_\theta L(\theta, \psi) \\
        \nabla_\psi L(\theta, \psi) - \nabla_\psi R_i(\theta, \psi)
        \end{pmatrix}
        $$
    </div>
    <div class="fragment">
        $$
        \Rightarrow
        \tilde v^\prime(\theta^*, \psi^*)
        = \begin{pmatrix}
        0 & -K_{DG}^\intercal \\
        K_{DG} & K_{DD} - L_{DD}
        \end{pmatrix}
        $$
    </div>
    <div class="fragment">
        <strong>Here</strong>: $K_{DD}$ negative definite and $K_{DG}$ has full column rank.
    </div>
    <div class="fragment">
        $\Rightarrow$ 
        All eigenvalues of 
        $\tilde v^\prime(\theta^*, \psi^*)$
        have negative real part
    </div>
</div>


