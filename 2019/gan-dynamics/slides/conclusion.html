
<!-- .slide: class="layout-1x1" -->

## Conclusion

<div class="column-1 flex-col">
    <ul style="width:80%; align-self: center;">
        <li class="fragment"> 
            <strong>GAN training</strong> belongs to a class of algorithms 
            that is <strong>more general</strong> than gradient descent
        </li>
        <li class="fragment">
            <strong>Convergence</strong> of these algorithms is <strong>no longer ensured</strong>
        </li>
        <li class="fragment">
            But we can use tools from <strong>discrete control theory</strong>
            to <strong>analyze</strong> and <strong>improve</strong> these algorithms
        </li>
        <li class="fragment">
            <strong>General theory</strong>
            that might be applicable to other systems, e.g. in reinforcement learning
        </li>
    </ul>
</div>

---

<!-- .slide: class="layout-1x1" -->

## References

<div class="column-1 flex-col">
    <ul style="width:90%; align-self: center; font-size: 90%">
        <li>
            <strong>L. Mescheder</strong>, S. Nowozin and A. Geiger.
            The Numerics of GANs
            <span>(NeurIPS 2017)</span>
        </li>
        <li>
            <strong>L. Mescheder</strong>, A. Geiger and S. Nowozin.
            Which Training Methods for GANs do actually Converge? 
            <span>(ICML 2018)</span>
        </li>
    </ul>
</div>

---

<div class="row-12 flex-row flex-center align-center" style="font-size: 2em">
    Thank you!
</div>