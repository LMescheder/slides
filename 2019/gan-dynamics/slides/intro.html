<!-- .slide: class="layout-1x1" -->

## Introduction

Discriminative neural networks:

<div class="column-1 flex-col" style="justify-content: space-evenly;">
    <div class="figure" style="height:600px; grid-template-columns: 33% 33% 33%;">
        <img src="gfx/gan/panda.jpg" width="80%"></img>
        <p></p>
        <img src="gfx/gan/network.svg" width="80%"></img>
        <p>Neural Network</p>
        <div>label $y$</div>
        <p></p>
    </div>
    <div style="height: 100px">
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Introduction

Generative neural networks:

<div class="column-1 flex-col" style="justify-content: space-evenly;">
    <div class="figure" style="height:600px; grid-template-columns: 33% 33% 33%;">
        <div class="subfig flex-col">
            <img src="gfx/gan/gaussian.svg" width="90%">
            <div>$z \sim \mathcal N(0, I_n)$</div>
        </div>
        <p></p>
        <img src="gfx/gan/network.svg" width="80%"></img>
        <p>Neural Network</p>
        <img src="gfx/gan/panda.jpg" width="80%"></img>
        <p></p>
    </div>
    <div class="fragment" style="height: 100px;">
        <strong>Key Challenge:</strong>
        have to learn high dimensional probability distribution
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Why generative models?

<p><strong>Reason 1</strong>: Many Problems are ill posed</p>

<div class="column-1 figure" style="height:800px;">
    <video src="gfx/gan2/nvidia_video.mp4" data-autoplay loop width=70%></video>
    <p>Wang et al. - Video-to-Video Synthesis (2018)</p>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Why generative models?

<p><strong>Reason 2</strong>: Data Augmentation</p>

<div class="column-1 figure" style="height:800px;">
    <img src="gfx/misc/aug-cars.svg" width=90%></img>
    <p>Alhaija, Mustikovela, Mescheder, Geiger, Rother - Augmented Reality Meets Computer Vision (IJCV 2018)</p>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Why generative models?

<p><strong>Reason 3</strong>: To learn flexible models</p>

<div class="column-1 figure" style="height:800px; grid-template-columns: 50% 50%;">
    <img src="gfx/misc/stutzgeiger.png" width=90%></img>
    <p>Stutz and Geiger - Learning 3D Shape Completion from Laser Scan Data with Weak Supervision (CVPR 2018)</p>
    <img src="gfx/misc/ranjan.png" width=90%></img>
    <p>Ranjan, Bolkart, Sanyal and Black - Generating 3D faces using Convolutional Mesh Autoencoder (ECCV 2018)</p>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Why generative models?

<p><strong>Reason 4</strong>: Art</p>

<div class="column-1 figure" style="height:800px;">
    <img src="gfx/misc/bethge.svg" width=90%></img>
    <p>Gatys, Ecker and Bethge - A Neural Algorithm of Artistic Style (2015)</p>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

Generative Adversarial Networks (GANs):

<div class="colmun-1 flex-col" style="justify-content: space-evenly">
    <img src="gfx/misc/gan-schematic.svg" width=80%></img>
    <div class="fragment">
        $\min_{\theta} \max_{\psi}
        \underbrace{
        \mathrm E_{p_0(z)} f \left(D_\psi(G_\theta(z))\right)
        + \mathrm E_{p_{\mathcal D} (x)} f\left(- D_\psi(x)\right)
        }_{=: L(\theta, \psi)}$
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

Timeline:

<div class="column-1 flex-col">
    <div class="figure" style="grid-template-columns: 33% 33% 33%; width: 90%;">
        <img src="gfx/misc/gan_t0.png" width=80%></img>
        <p>[Goodfellow et al., 2014]</p>
        <img src="gfx/misc/gan_t1.png" width=80%></img>
        <p>[Radford & Metz, 2016]</p>
        <img src="gfx/misc/gan_t2.png" width=80%></img>
        <p>[Karras et al., 2018]</p>
    </div>
    <img src="gfx/misc/arrow.svg" width=90%></div>
</div>

---

<!-- .slide: class="layout-2x1" -->

## Generative Adversarial Networks

<div class="column-1">
    <h3>Generative Adversarial Networks:</h3>
    <ul>
        <li class="fragment">We can now achieve impressive results</li>
        <li class="fragment">However, training is still difficult and requires many heuristics</li>
    </ul>
</div>

<div class="column-2 fragment">
    <h3>Questions:</h3>
    <ul>
        <li class="fragment">What are the underlying principles?</li>
        <li class="fragment">Can we give theoretical guarantees for convergence?</li>
        <li class="fragment">Can we derive new families of algorithms?</li>
    </ul>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

<h3>Alternating Gradient Descent:</h3>

<div class="column-1 flex-col">
    <img src="gfx/misc/altgd.svg" width="40%"></img>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

<h3>Simultaneous Gradient Descent:</h3>

<div class="column-1 flex-col">
    <img src="gfx/misc/simgd.svg" width="40%"></img>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

<h3>What are we doing here?</h3>

<div class="column-1 flex-col" style="justify-content: space-evenly;">
    <ul>
        <li class="fragment">We no longer solve an optimization problem</li>
        <li class="fragment">Instead, we solve a smooth two player</li>
    </ul>
    <div class="flex-row fragment" 
        style="width: 70%; align-self: center; text-align: left; padding: 20px; border: 10px solid skyblue; border-radius: 30px;">
        <img src="gfx/misc/arrow2.svg" width="20%">
        <div style="padding: 0px 50px;">
            Standard convergence results for stochastic gradient descent hence no longer apply
        </div>
    </div>
</div>

---

<!-- .slide: class="layout-1x1" -->

## Generative Adversarial Networks

<h3>Questions:</h3>

<div class="column-1">
    <ol style="width: 80%">
        <li class="fragment">
            Does a (pure) Nash-equilibrium exist?
            <ul class="fragment"><li>
                Yes, if there is $\theta$ with $p_\theta = p_{\mathcal D}$ (Goodfellow et al., 2014)
            </li></ul>
        </li>
        <li class="fragment">
            Does it solve the min-max problem?
            <ul class="fragment"><li>
                Yes, if $p_\theta = p_{\mathcal D}$ (Goodfellow et al., 2014)
            </li></ul>
        </li>
        <li class="fragment">
            <div style="border: 10px solid skyblue; border-radius: 20px; padding: 10px;">
                Do simultaneous and / or alternating gradient descent converge to the Nash-equilibrium?
            </div>
        </li>
    </ol>
</div>
